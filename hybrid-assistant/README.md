# ğŸš€ Hybrid Assistant
![AsistentAI](ikona.png)

A smart assistant combining **video recognition** and **LLM** (Large Language Model) for advanced object detection, chat, and web search.

---

## âœ¨ Features

- **ğŸ¥ Video Recognition:** Detect objects in real-time using YOLO/Ultralytics.
- **ğŸ¤– LLM Integration:** Use llama-cpp-python for chat, reasoning, and custom commands.
- **ğŸ” DuckDuckGo Search:** Search the web directly from chat.
- **ğŸ†• Custom Object Addition:** Add new objects for detection via chat.
- **ğŸ’¬ Chat Interface:** Interact with the assistant, ask questions, and control features.
- **ğŸ“¸ Camera Control:** 
  - Turn camera **on**: `turn cam on` (calls camera open function)
  - Turn camera **off**: `turn cam off` (calls camera close function)
- **âš¡ .bat Loader:** Easy startup with provided batch script.
- **ğŸ“¦ Requirements Management:** All dependencies listed in `requirements.txt`.

---

## ğŸ› ï¸ Usage

1. **Install dependencies:**  
   `pip install -r requirements.txt`
2. **Run the assistant:**  
   `run.bat` or `python main.py`
3. **Interact via chat:**  
   - Ask questions about detected objects.
   - Search the web: `search <query>`
   - Add custom objects: `add object <name>`
   - Control camera: `turn cam on` / `turn cam off`

---

## ğŸ¥ DuckDuckGo Search

- Use chat to search the web:  
  `search <your query>`

## ğŸ†• Custom Object Add

- Add new objects for detection:  
  `add object <object_name>`

## ğŸ“¸ Camera Control

- **Turn On:**  
  `turn cam on`
- **Turn Off:**  
  `turn cam off`

---

## âš ï¸ Troubleshooting

- **pip version warning:**  
  Update pip: `python -m pip install --upgrade pip`
- **Matplotlib warning:**  
  Ensure compatible versions in `requirements.txt`.

---

## ğŸ“‚ Project Structure

- `main.py` â€” Main entry point
- `video_recognition.py` â€” Object detection logic
- `llm_chat.py` â€” LLM chat integration
- `duckduck_search.py` â€” DuckDuckGo search
- `requirements.txt` â€” Dependencies
- `run.bat` â€” Startup script

---

## Note on DuckDuckGo Search Library

As of November 2025, the `ddgs` package (the successor to `duckduckgo-search`) could not be installed or imported in this environment. Therefore, the project continues to use `duckduckgo-search` for internet search features. If you see a deprecation warning, it is safe to ignore for now. Future updates may switch to `ddgs` if compatibility improves.

## Hybrid Assistant

A local, privacy-focused **face recognition and chatbot** application that combines camera-based visual detection with a local LLM backend. Teach the assistant to recognize faces and objects through the GUI or chat, get intelligent scene descriptions, and have conversationsâ€”all running locally and offline.

Developed by **@daqa020** with â¤ï¸

## Features

- **Live Camera GUI** â€” Real-time video feed with object detection overlays and interactive chat panel
- **Visual Learning** â€” Teach new objects by drawing rectangles/polygons on the camera feed
- **Chat-Based Teaching** â€” Use simple commands like `Teach: object_name - description`
- **Face Recognition** â€” OpenCV LBPH-based face detection and recognition (optional)
- **Embeddings** â€” CLIP-based visual embeddings for robust object matching
- **Local LLM** â€” Ollama HTTP API integration for text generation
- **Privacy-First** â€” All data stored locally; no cloud dependency

## Quick Start

### Prerequisites

- Python 3.11+
- Camera (webcam or USB)
- Ollama installed and running (see [INSTRUCTIONS.md](INSTRUCTIONS.md) for setup)

### âš¡ Getting Started (Windows)

1. **Clone the repository:**
   ```bash
   git clone https://github.com/sarahubanic/hybrid-assistant.git
   cd hybrid-assistant
   ```

2. **That's it! Just double-click `run.bat`**
   - The script will automatically:
     - Create a virtual environment
     - Install all dependencies
     - Ask you to choose a mode (CPU / CUDA / Hybrid)
     - Start the app

3. **Start Ollama** (in a separate terminal/window):
   ```bash
   ollama serve
   ```

### Manual Setup (Advanced)

If you prefer manual control:
```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run the app
python detection_gui.py
```

### Linux / macOS

```bash
git clone https://github.com/sarahubanic/hybrid-assistant.git
cd hybrid-assistant

# Create and activate venv
python -m venv venv
source venv/bin/activate

# Install and run
pip install -r requirements.txt
python detection_gui.py
```

## Usage

### Main GUI (`detection_gui.py`)

- **Camera View**: Live feed with detection overlays
- **Chat Panel**: Send messages or teach commands
- **Teach Dialog**: Open to draw rectangles/polygons around objects to learn

### Teaching an Object

1. Click **"Teach"** button to open the Teach dialog
2. Freeze the camera with the object visible
3. Draw a **rectangle** or **polygon** around the object
4. Add a name and optional description
5. Click **Save** â€” the object is now learned

### Chat Commands

- **`Teach: object_name - description`** â€” Teach the assistant using text only. Example:
   - `Teach: Umbrella - A collapsible black umbrella commonly stored in a bag`
   - The assistant will add this to the local knowledge base and use it for recognition and replies.

- **`teach:`** and **`Teach:`** are treated equivalently (case-insensitive) by the GUI.

- **`Search: query`** or **`pretrazi: query`** â€” Perform a DuckDuckGo search and return concise results (title, short snippet, URL).
   - Example: `Search: current weather in London`
   - Example (Serbian): `pretrazi: vreme u Beogradu`

- **`Add object` (GUI)** â€” Use the Teach dialog to visually add objects:
   1. Click **"Teach Me Something"**.
   2. Freeze the camera frame when the object is visible.
   3. Draw a **rectangle** or **polygon** around the object (or use face mode for faces).
   4. Enter a **Name** and **Description** and click **Save**. The object and its CLIP embedding will be stored in `learned_items/`.

- **`Teach:` (chat + image)** â€” For future improvements the app will support `Teach:` combined with an attached image or `Send + Img` button to teach from a crop; currently, use the Teach dialog for visual teaching.

## Files

| File | Purpose |
|------|---------|
| `detection_gui.py` | Main GUI application |
| `hybrid_assistant.py` | Core assistant logic |
| `mistral_chatbot.py` | Ollama chat integration |
| `ollama_client.py` | Ollama HTTP client |
| `video_assistant.py` | Video processing utilities |
| `requirements.txt` | Python dependencies |
| `INSTRUCTIONS.md` | Detailed setup & configuration |

## Configuration

See [INSTRUCTIONS.md](INSTRUCTIONS.md) for:
- Ollama model recommendations
- Face recognition setup
- Visual embedding configuration
- Troubleshooting

## Project Structure

```
hybrid-assistant/
â”œâ”€â”€ detection_gui.py          # Main GUI
â”œâ”€â”€ hybrid_assistant.py        # Core logic
â”œâ”€â”€ mistral_chatbot.py         # Chat backend
â”œâ”€â”€ ollama_client.py           # LLM client
â”œâ”€â”€ video_assistant.py         # Video utilities
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ INSTRUCTIONS.md            # Setup guide
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ learned_items/             # Persistent knowledge
â”‚   â”œâ”€â”€ knowledge.json         # Learned facts
â”‚   â””â”€â”€ visual_samples/        # Saved object crops
â”œâ”€â”€ logs/                      # Run logs
â””â”€â”€ models/                    # Model cache (optional)
```

## Privacy & Data

- âœ… Runs entirely on your machine
- âœ… No cloud calls or telemetry
- âœ… All learned data stored locally in `learned_items/`
- âœ… Optional face recognition (local only)

## License

MIT License â€” See [LICENSE](LICENSE) for details.

## ğŸ™ Special Thanks

This project was made possible thanks to:

- **[@jinnosux](https://github.com/jinnosux)** â€” For providing the hardware tools and the original idea that inspired this project. Your vision made this possible! ğŸš€

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

**Last Updated**: November 17, 2025

## ğŸ§  LLM Backend Selection

- **GROQ Backend:**
  - If selected without a configured model, chat is disabled and a warning is shown.
  - To enable chat, configure a GROQ model in settings.
