# Hybrid Assistant

A local, privacy-focused **face recognition and chatbot** application that combines camera-based visual detection with a local LLM backend. Teach the assistant to recognize faces and objects through the GUI or chat, get intelligent scene descriptions, and have conversationsâ€”all running locally and offline.

Developed by **@daqa020** with â¤ï¸

## Features

- **Live Camera GUI** â€” Real-time video feed with object detection overlays and interactive chat panel
- **Visual Learning** â€” Teach new objects by drawing rectangles/polygons on the camera feed
- **Chat-Based Teaching** â€” Use simple commands like `Teach: object_name - description`
- **Face Recognition** â€” OpenCV LBPH-based face detection and recognition (optional)
- **Embeddings** â€” CLIP-based visual embeddings for robust object matching
- **Local LLM** â€” Ollama HTTP API integration for text generation
- **Privacy-First** â€” All data stored locally; no cloud dependency

## Quick Start

### Prerequisites

- Python 3.11+
- Camera (webcam or USB)
- Ollama installed and running (see [INSTRUCTIONS.md](INSTRUCTIONS.md) for setup)

### âš¡ Getting Started (Windows)

1. **Clone the repository:**
   ```bash
   git clone https://github.com/sarahubanic/hybrid-assistant.git
   cd hybrid-assistant
   ```

2. **That's it! Just double-click `run.bat`**
   - The script will automatically:
     - Create a virtual environment
     - Install all dependencies
     - Ask you to choose a mode (CPU / CUDA / Hybrid)
     - Start the app

3. **Start Ollama** (in a separate terminal/window):
   ```bash
   ollama serve
   ```

### Manual Setup (Advanced)

If you prefer manual control:
```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run the app
python detection_gui.py
```

### Linux / macOS

```bash
git clone https://github.com/sarahubanic/hybrid-assistant.git
cd hybrid-assistant

# Create and activate venv
python -m venv venv
source venv/bin/activate

# Install and run
pip install -r requirements.txt
python detection_gui.py
```

## Usage

### Main GUI (`detection_gui.py`)

- **Camera View**: Live feed with detection overlays
- **Chat Panel**: Send messages or teach commands
- **Teach Dialog**: Open to draw rectangles/polygons around objects to learn

### Teaching an Object

1. Click **"Teach"** button to open the Teach dialog
2. Freeze the camera with the object visible
3. Draw a **rectangle** or **polygon** around the object
4. Add a name and optional description
5. Click **Save** â€” the object is now learned

### Chat Commands

- **`Teach: object_name - description`** â€” Teach from text
- Regular chat â€” Ask questions about what the camera sees

## Files

| File | Purpose |
|------|---------|
| `detection_gui.py` | Main GUI application |
| `hybrid_assistant.py` | Core assistant logic |
| `mistral_chatbot.py` | Ollama chat integration |
| `ollama_client.py` | Ollama HTTP client |
| `video_assistant.py` | Video processing utilities |
| `requirements.txt` | Python dependencies |
| `INSTRUCTIONS.md` | Detailed setup & configuration |

## Configuration

See [INSTRUCTIONS.md](INSTRUCTIONS.md) for:
- Ollama model recommendations
- Face recognition setup
- Visual embedding configuration
- Troubleshooting

## Project Structure

```
hybrid-assistant/
â”œâ”€â”€ detection_gui.py          # Main GUI
â”œâ”€â”€ hybrid_assistant.py        # Core logic
â”œâ”€â”€ mistral_chatbot.py         # Chat backend
â”œâ”€â”€ ollama_client.py           # LLM client
â”œâ”€â”€ video_assistant.py         # Video utilities
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ INSTRUCTIONS.md            # Setup guide
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ learned_items/             # Persistent knowledge
â”‚   â”œâ”€â”€ knowledge.json         # Learned facts
â”‚   â””â”€â”€ visual_samples/        # Saved object crops
â”œâ”€â”€ logs/                      # Run logs
â””â”€â”€ models/                    # Model cache (optional)
```

## Privacy & Data

- âœ… Runs entirely on your machine
- âœ… No cloud calls or telemetry
- âœ… All learned data stored locally in `learned_items/`
- âœ… Optional face recognition (local only)

## License

MIT License â€” See [LICENSE](LICENSE) for details.

## ğŸ™ Special Thanks

This project was made possible thanks to:

- **[@jinnosux](https://github.com/jinnosux)** â€” For providing the hardware tools and the original idea that inspired this project. Your vision made this possible! ğŸš€

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

**Last Updated**: November 17, 2025
